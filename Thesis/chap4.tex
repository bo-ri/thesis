\chapter{検証実験}
モデルの精度がどの程度妥当かを検証するために，本章ではパラメータを微調整したモデルの出力結果をまとめる．
またそれぞれの出力結果に関して評価する．

\section{実験概要}
検証実験では，2通りの方法で生成したモデルを用いて出力を得た．
1つはWord2Vecで生成したモデルで，もう一方はGloVeで生成したモデルである．
また，それぞれの方法でモデルを生成する際に，異なるパラメータを適用していくつかのモデルを学習した．
適用したパラメータの詳細は次節で説明する．

出力した内容は，モデルに対して大学名を入力とし，入力された大学に近い大学名を出力とした．
また，入力と出力の2つの大学間において近さを定義する共通の単語について，それぞれの大学からの距離を示した．
例として，A大学に近い大学としてB大学が得られた場合，A大学とB大学で共通して近い単語を探す．
得られた単語からのそれぞれの大学間の距離が近ければ，それぞれの大学が近い要因としての単語を得ることができる．

モデルを用いて出力する内容は，青山学院大学に近い大学名， $ 青山学院大学 - キリスト教 $ に該当する大学名，青山学院大学と明治学院大学それぞれで共通する意味が近い単語の3項目とした．
明治学院大学と比較した理由は，同じミッション系の大学で神学部の統合を通した日本神学校の創立などの関係性を持っているためである．

\section{評価軸}
モデルを評価する際に，それぞれの出力に対して評価軸を定めた．

青山学院大学に近い大学に関する評価項目は，まず偏差値が近いこと，ミッション系の大学であること，キャンパスの立地の近さである．
本研究で参考にした偏差値とキャンパスの立地はパスナビ\cite{passNavi}を参考とした．
モデルに学部，学科の情報は考慮されていないため，キャンパスの近さは学部を考慮しない．

次に，$ 青山学院大学 - キリスト教 $ の評価項目は，非ミッション系の大学であること，偏差値が近いこと，キャンパスの立地が近いこととした．
この場合，大学の要素からはキリスト教が消えていることが期待されるため，非ミッション系の大学であることをもっとも重要な評価項目とする．

最後に，青山学院大学と明治学院大学で共通の近い単語の評価項目は，直接的な関係性を示す単語とした．
これは，両校がミッション系であることから，キリスト教に関する単語などがあげられる．
また，歴史的な背景から日本神学校(現 東京神学大学)の創設に両校の神学部が統合したことから，神学部に関する単語も考慮する．

\begin{table}[htbp]
\caption{各出力結果の評価軸}
\centering
\begin{tabular}{|l|l|l|}
\hline
青山学院大学に近い大学 & $ 青山学院大学 - キリスト教 $ & 青山学院大学と明治学院大学で共通の近い単語
\\ \hline \hline
偏差値が近い& 非ミッション系 & 直接的な関係性を示す単語 \\
ミッション系& 偏差値が近い & \\
立地 & 立地 & \\ \hline
\end{tabular}
\label{table:eval}
\end{table}

\section{パラメータの詳細}
各パラメータの詳細について解説する．

\subsection{単語ベクトルの次元数}
% word2vecとGloVeで単語ベクトルを学習する際に指定するsizeオプションでは，隠れ層の単語ベクトルの次元数を指定する．
% このオプションで指定したサイズ * 全体の単語数のサイズのベクトルに全ての単語を圧縮し，分散表現を得る．
% この次元数が大きすぎると効率的な分散表現を学習できないが，次元数が小さすぎると単語の特徴を十分にとらえきれなくなり，学習に時間を要する．
Word2VecとGloVeで指定する次元数は，小さすぎると単語の特徴を効率的に学習できず，大きすぎると適切な分散表現が学習できない．
一般的に，50 ~ 300次元を指定する．
本研究で使用したデータセットは比較的サイズが小さいため，単語ベクトルの次元数は50次元とした．

\subsection{反復回数}
% iterオプションでは，学習の反復回数を指定する．
% 反復回数が少ないと，最適な分散表現が得られる前に学習が終了してしまう．
% また，反復回数を増やすと学習に要する時間が増加する．
% 検証実験では，最適な反復回数として，100 ~ 1000回を比較対象とする．
Word2VecとGloVeのトレーニングの反復回数を指定する．
この数字の大きさに比例して学習に要する時間も大きくなる．
また，反復回数が少なすぎると十分に単語の特徴を学習できないため，検証実験では10, 100, 1000のパラメータで比較する．

\subsection{windowサイズ}
% ある単語の単語ベクトルを学習する際に，文書に出現した学習対象の単語から指定した単語数まで離れた単語を対象として学習する．
% windowオプションではこの単語数を指定する．
% 本研究で用いた学習データは，大学に関するプレス記事を用いたため，windowサイズは1000とした．
% これは，記事の中に出現する大学名の単語ベクトルを学習する際，対象となる単語は記事全体に出現すると考えられるためである．
windowサイズは10と1000で比較した．
一般的にはwindowサイズは10 ~ 20で学習するが，記事の中に出現する大学名の単語ベクトルを学習する際，対象となる単語は記事全体に出現すると考えられるためwindowサイズに1000を適用して比較する．

\subsection{x-max}
GloVeの学習を行う際に，共起頻度の閾値を指定する必要がある．
Jeffrey Penningtonらの実験では，100,000,000 ~ 600,000,000個のトークンが含まれたコーパスを用いて，x-maxオプションに100を指定した．
一方本論文で学習したデータは約2,400,000個のトークンが含まれたデータを用いたため，x-maxオプションに指定する値は10と5で比較する．


\input{word2vecResult}
\input{gloveResult}