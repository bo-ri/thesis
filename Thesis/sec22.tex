\section{GloVe}
本節ではGloVeについて解説する．

\subsection{概要}
GloVeはJeffrey Penningtonら\cite{glove}によって提案された単語ベクトルを取得する実装である．
Word2Vecよりも後に提案された手法であり，C言語で実装されている．
GloVeでは，コーパス全体の単語間の共起の数を最小二乗法で学習するモデルとなっている．
そのため，Word2Vecと比較して学習時間が短縮できる．
また，小さなコーパスでも学習ができ，精度の高さも論文の実験から得られている．
Word2Vecでは考慮できない，出現回数が極端に少ない単語に重要度が偏るといった問題も回避できる．
これはGloVeでは共起頻度が極端に高い単語と，低い単語を重視しないためである．

\subsection{パラメータ}
基本的なパラメータはWord2Vecと共通であるため割愛する．
ここでは本研究で適用したx-maxオプションの値について説明する．
x-maxオプションはGloVeの学習の際に指定するパラメータで，共起頻度の閾値を表す．
(2.1)式で示すように，重み関数 $ f(x) $ は2つの単語 $ {\rm i, j} $ の共起頻度がx-maxオプションで指定した値未満の時に 
$ (x/x_{max})^\alpha $ となり，それ以上の場合は $ 1 $ となる．
$ \alpha $ の値は論文から $ 0.75 $ とする．
\begin{equation}
f(x) = \left\{\begin{array}{l}
(x/x_{max})^\alpha \;\;\; {\rm if} \; x < x_{max} \\
\;\;\;\;\; 1 \;\;\;\;\;\;\;\;\;\;\;\;\; {\rm otherwise}
\end{array} \right.
\end{equation}
これにより，共起頻度が低い単語は重みが低くなり，共起頻度の閾値以上の単語は重みが1になる．

% 論文ではx-maxオプションを100に指定して実験を行なっていた．
% しかし本研究で使用したデータサイズは，論文で用いられたデータサイズよりも小さいため，x-maxに指定する値は 5 とした．