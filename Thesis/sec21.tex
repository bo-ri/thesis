\section{Word2Vec}
本節ではWord2Vecの概要とパラメータについて説明する．
Word2Vec\cite{word2vecBook}は，Tomas Mikolovら\cite{word2vec}によって提案された，単語をベクトルに変換するためのニューラルネットワークの実装である．
単語をベクトルで表現することで，単語同士の関連性を定量的に扱うことができる．
またベクトルに変換することで，単語同士でベクトルの距離の足し引きができるようになるため，単語の演算が可能になる．

\subsection{Word2Vecの構造}
Word2Vecの構造は入力層，隠れ層，出力層からなる単純なニューラルネットワークとなっている．
入力層と出力層は学習する単語の数だけ存在する．
隠れ層はあらかじめ指定した次元数×単語数(入力層の数)のベクトルからなる．

入力層で受ける入力は文章を1-of-K形式に変換したものとなり，出力結果が最適になるように隠れ層の単語ベクトルの重みを学習する．
最終的に得られるモデルはこの隠れ層で学習した単語ベクトルになる．

\subsection{単語ベクトルの次元数}
word2vecで単語ベクトルを学習する際に指定するsizeオプションでは，隠れ層の単語ベクトルの次元数を指定する．
このオプションで指定したサイズ * 全体の単語数のサイズのベクトルに全ての単語を圧縮し，分散表現を得る．
この次元数が大きすぎると効率的な分散表現を学習できないが，次元数が小さすぎると単語の特徴を十分にとらえきれなくなり，学習に時間を要する．

\subsection{反復回数}
反復回数が少ないと，最適な分散表現が得られる前に学習が終了してしまう．
また，反復回数を増やすと学習に要する時間が増加する．
検証実験では，最適な反復回数として，100 ~ 1000回を比較対象とする．

\subsection{windowサイズ}
ある単語の単語ベクトルを学習する際に，文書に出現した学習対象の単語から指定した単語数まで離れた単語を対象として学習する．
windowオプションではこの単語数を指定する．
% 本研究で用いた学習データは，大学に関するプレス記事を用いたため，windowサイズは1000とした．
% これは，記事の中に出現する大学名の単語ベクトルを学習する際，対象となる単語は記事全体に出現すると考えられるためである．


\subsection{本研究での利用}
本研究で構築するシステムでは，大学間の関連を分析するためにWord2Vecを用いる．
具体的には，大学プレスセンター\cite{pressCenter}の記事から，あらかじめリストアップした大学に関する記事をスクレイピングにより取得し，
それぞれの記事を学習データとしてWord2Vecのモデルを学習した．